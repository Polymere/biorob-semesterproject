#!/usr/bin/env python
""" @package controller
Optimisation controller

"""
import sys
import yaml
import numpy as np
import pandas as pd
import os
import time

from shutil import copyfile

from run_batch_controller.run_launcher import CppLauncher,PythonLauncher

from utils.file_utils import assert_file_exists,assert_dir

from data_analysis.process_run import CppRunProcess,PythonRunProcess

from optimisation.optimizers import PSOptimizer,GAOptimizer,NSGAIIOptimizer

NORUNMODE=False
if NORUNMODE:
	ROOT_RESULT_DIR = "./trash"
else:
	ROOT_RESULT_DIR = "../../data/runs" ## PATH

import warnings
warnings.filterwarnings('ignore') # pandas warning a utter trash

LOG_DEBUG=1
LOG_INFO=2
LOG_WARNING=3
LOG_ERROR=4
LOG_LEVEL=LOG_INFO


class EvolutionController:
	"""Parent class for evolution controller
			Attributes:
			- run_launcher: runLauncher object, depending on model implementation (python vs cpp)
			- run_processor: runProcess object, depending on model implementation (python vs cpp)
			- max_nb_gen: maximum generation number (default=100)
			- nb_ind=10: number of inidviduals per generation (default=10)
			- dofs: degrees of freedom of the optimisation (dict formated as 
				{param_name1 : [bound_low1,bound_high1],
				param_name2 : [bound_low2,bound_high2],...
				} )
			- optimizer_name: name of the optimisation method (either PSO,GA or NSGAII)
			- init_pop_mode: initialisation method for the population (see set_init_pop)
	"""
	
	run_launcher=None
	run_processor=None
	max_nb_gen=100
	nb_ind=10
	dofs=None

	optimizer_name="NSGAII"
	init_pop_mode="rand"
	# Random values within boundaries
	
	def __init__(self, ev_config):
		""" Evolution controller initialisation
				
				Initialization of the evolution controller arguments, creation of
				the optimizer object and the trial directory, where the relevant 
				optimisation parameters are saved
				Input:
				- ev_config: dict containing all configuration for the evolution,
					read from the .yaml file specified at launch 
					(see ev_config_template.yaml)				
				Dependencies:
				optimizers
				
		"""
		self.__dict__.update(ev_config)

		self.trial_dir = os.path.join(ROOT_RESULT_DIR, time.strftime("%j_%H:%M"))
		assert_dir(self.trial_dir,should_be_empty=True)
		with open(os.path.join(self.trial_dir,"ev_params.yaml"),"w+") as outparams:
			yaml.dump(ev_config,outparams,default_flow_style=False)

		if self.optimizer_name=="NSGAII":
			self.union_pop=True
			self.opti=NSGAIIOptimizer(ev_config)
		elif self.optimizer_name=="GA":
			self.union_pop=False
			self.opti=GAOptimizer(ev_config)
		elif self.optimizer_name=="PSO":
			self.union_pop=False
			self.opti=PSOptimizer(ev_config)
		else:
			if LOG_LEVEL<=LOG_ERROR:
				print("\n[ERROR]Unknown optimizer name\n",self.optimizer_name)
			raise ValueError

		self.nb_gen=0
		self.flatten_params()
		self.current_pop=None

	def set_init_pop(self,initial_pop=None):
		""" Creation of the initial population, according the specified method

				Creates a Dataframe with nb_ind rows and nb_dofs columns,
				corresponding to the population size and the number of degrees
				of freedom.
				The initialization mode can either be:
				- rand : randow value within boundaries for each parameter
				- one_parent : Mutation of an initial parent (parameter set) according
					to the optimizers get_next_gen method
				- multiple_parents : Mutation/crossover of an initial population
					according to the optimizer get_next_gen method. Can be used 
					to resume an evolution from a certain generation. Note that
					no selection/sorting is applied on the parent population

				\param initial_pop
				- mode == rand  : None, only need the dofs names and boundaries to be set
				- mode == one_parent : dict with format 
										{param_name1 : param_value1,
										...}
				- mode == multiple_parents : path to a csv file, typically the genX.csv
					file created at the end of another optimisation
				Output:
				First generation to evaluate	
		"""

		uids=["gen"+str(self.nb_gen)+"ind"+str(i+1) for i in range(self.nb_ind)]
		pop_df=pd.DataFrame(index=uids,columns=self.dofs.keys())
		if self.init_pop_mode=="rand":
				vals=(self._bound_high-self._bound_low)*np.random.rand(self.nb_ind,len(self._bound_low)) - self._bound_low
				pop_df[:]=vals
				return pop_df
		elif self.init_pop_mode=="one_parent":
			"""
				initial population is generated by mutating a reference parent (working initial parameter set)
				"""
			if initial_pop is None:
				raise ValueError
			print("\n[INFO]Initializing population from parent\n",initial_pop)
			for index, row in pop_df.iterrows():
				pop_df.loc[index]=initial_pop
			select_ids=pop_df.head(1)
			return self.opti.get_next_gen(select_ids,self.nb_gen)
		elif self.init_pop_mode=="multiple_parents":
			""" 
				initial population is generated by mutation and crossover of a full parent population
				Used to resume evolution from a certain generation
				"""
			if initial_pop is None:
				raise ValueError
			parents=pd.read_csv(initial_pop)
			return self.opti.get_next_gen(parents,self.nb_gen)
		else:
			if LOG_LEVEL<=LOG_ERROR:
				print("\n[ERROR]Unknown initialization mode :",self.init_pop_mode)
			raise KeyError
			

	def evolve(self):
		"""Main optimization loop. 
				
				1. Generate initial population
				2. Evaluate population (simulation)
				3. Choose parents (sorting  + selection in optimizer)
				4. Get next generation from parents (in optimizer)
				5. Save current parents and delete previous parents (in order to be able 
					to resume optimization if stopped)
				6. If max generation is not reached, back to 2

				Dependencies:
				optimizers
		"""
		childrens=self.set_init_pop(self.initial_pop)
		parents=None
		while not self.is_stop():
			print("\n[INFO]***************GEN",self.nb_gen,"***************\n")
			eval_pop=self.eval_pop(childrens)
			if self.union_pop and parents is not None:
				candidates=pd.concat([eval_pop,parents],axis=0)
			else:
				candidates=eval_pop
			parents=self.opti.select(self.opti.sort_pop(candidates),self.nb_gen)
			if LOG_LEVEL<=LOG_INFO:
				print("\n[INFO]Gen",self.nb_gen," Parents\n",parents)
			self.nb_gen+=1
			childrens=self.opti.get_next_gen(parents,self.nb_gen)
			#saves current parents and removes previous parents
			parents.to_csv(os.path.join(self.trial_dir,"parents_gen"+\
				str(self.nb_gen)+".csv"))
			if self.nb_gen>2:
				os.unlink(os.path.join(self.trial_dir,"parents_gen"+\
					str(self.nb_gen-1)+".csv"))


	def eval_pop(self,population):
		""" Evaluation of the population. 

			1. Writes the parameter values of each induvidual of the population to file
				in param_paths
			2. Run all the worlds in worlds_path
			3. Once all the runs are finished, process logged values in log_paths to get
				fitness dataframe
			4. Merge population and evaluated fitness according to uid
			5. Save results
			6. Add the stability evaluation (1 if stable, - 1000 if not) to other fitness 
				measures in order to penalize unstable trials
			7. Return evaluated population for optimization

			\param population -- Dataframe with rows for each individual, 
			 	with uid and columns being the dofs (models parameters). Columns names
			 	must be consistent with the parameters names used for simulation !
			Output:
			evaluated population -- population dataframe, with additional columns 
				corresponding to relevant fitnesses (in objective_metrics)
			Dependencies : 
			run_launcher
			process_run
		"""
		if LOG_LEVEL<=LOG_INFO:
			print("\n[INFO]Evaluating population\n", population)
		self.run_launcher.create_pop(population.to_dict(orient="index"))

		log_path=self.run_launcher.run_batch("worlds")

		scores=self.run_processor.get_fitness_from_dir(log_path)
		population["gen"]=self.nb_gen
		if LOG_LEVEL<=LOG_DEBUG:
			print("\n[DEBUG]Pop\n",population)
			print("\n[DEBUG]scores\n",scores)

		saved_df=pd.concat([population,scores],axis=1)
		saved_df.to_csv(os.path.join(self.trial_dir,"gen"+str(self.nb_gen)+".csv"))

		if self.opti.is_single_obj:
			scores=scores.add(scores.fit_stable,axis='index')
			scores["fitness"]=scores.filter(self.objectives_metrics).sum(1)
			evaluated=pd.concat([population,scores.fitness],axis=1)
		else:
			scores=scores.add(scores.fit_stable,axis='index')
			evaluated=pd.concat([population,scores.filter(self.objectives_metrics)],axis=1)

		if LOG_LEVEL<=LOG_DEBUG:
			print("\n[DEBUG]Evaluated pop\n",evaluated)
		return evaluated

	def is_stop(self):
		"""Stop condition check
				Return True if max generation number has been reached
		"""
		if self.nb_gen>self.max_nb_gen:
			return True
	def flatten_params(self):
		""" Transformation of keyvals to arrays

			Transforms the dofs dict 
				- {	param_name1 : [bound_low1,bound_high1],
					param_name2 : [bound_low2,bound_high2],...
				  } 
			to
				- dofs_names: 	[param_name1,param_name2 ...] 	(list)
				- _bound_low: 	[bound_low1,bound_low2,...] 	(np array)
				- _bound_high: 	[bound_high1,bound_high2,...] 	(np array)
			for ease of access / parallel operations in population
			initialization and boundary checks
		"""
		nb_par=len(self.dofs)
		self._bound_low=np.empty(nb_par, np.float16)
		self._bound_high=np.empty(nb_par, np.float16)
		self._params=[]
		self.dof_names=[]
		i=0
		for dof_name,dof_bounds in self.dofs.items():
			self._bound_low[i]=dof_bounds[0]
			self._bound_high[i]=dof_bounds[1]
			self.dof_names.append(dof_name)
			i+=1


class CppEvolutionController(EvolutionController):
	""" Evolution controller for the Cpp implementation of the locomotion model,
		with the specific run launcher and run processor objects

		Dependencies:
		CppLauncher
		CppRunProcess
	"""
	def __init__(self,config_file):
		params=yaml.load(open(config_file, 'r'))
		EvolutionController.__init__(self,params)
		if hasattr(self, "trial_dir"): 
		# little hack to avoid recreating the trial directory in CppLauncher
			params["trial_dir"]=self.trial_dir
		self.run_launcher=CppLauncher(params)
		self.run_processor=CppRunProcess(params)

class PythonEvolutionController(EvolutionController):
	"""Evolution controller for the python implementation of the locomotion model 
	
		WARNING : Not tested, the project was developed with the cpp implementation
		However, the structure of the controller should be the same. 
		Differences would be in PythonLauncher and PythonRunProcess, declared namely
		in run_launcher.py and process_run.py

		Dependencies:
		PythonLauncher
		PythonRunProcess
	"""
	def __init__(self,config_file):
		params=yaml.load(open(config_file, 'r'))
		EvolutionController.__init__(self,params)
		if hasattr(self, "trial_dir"):
			# little hack to avoid recreating the trial directory in PythonLauncher
			params["trial_dir"]=self.trial_dir
		self.run_launcher=PythonLauncher(params)
		self.run_processor=PythonRunProcess(params)

def launch_opt(ev_file,model):
	""" Launching optimisation from terminal

		run from terminal as 
		'python controller.py EV_CONFIG_FNAME (model)'
		with EV_CONFIG_FNAME the name of the configuration file (.yaml),
		located in repo/data/references (see ev_config_template.yaml)
		and model an optional argument specifying which model to use / debug mode
		(cpp model without debug messages by default)
	"""
	if model=="cpp":
		config_file=os.path.join("../../data/ev_config",ev_file)
		assert_file_exists(config_file, should_exist=True)
		c=CppEvolutionController(config_file)
		c.evolve()

	elif model=="cpp_debug":
		config_file=os.path.join("../../data/ev_config",ev_file)
		assert_file_exists(config_file, should_exist=True)
		c=CppEvolutionController(config_file)
		LOG_LEVEL=LOG_DEBUG
		c.evolve()
	elif model=="py":
		config_file=os.path.join("../../data/ev_config",ev_file)
		assert_file_exists(config_file, should_exist=True)
		c=PythonEvolutionController(config_file)
		c.evolve()
	elif model=="py_debug":
		config_file=os.path.join("../../data/ev_config",ev_file)
		assert_file_exists(config_file, should_exist=True)
		c=PythonEvolutionController(config_file)
		LOG_LEVEL=LOG_DEBUG
		c.evolve()
	else:
		raise AttributeError


if __name__ == '__main__':

	ev_file=sys.argv[1]
	try:
		model=sys.argv[2]
	except:
		model="cpp"
	try:
		launch_opt(ev_file, model)
	except AttributeError:
		if LOG_LEVEL<=LOG_ERROR:
			print("[ERROR]Wrong input arguments :\n",launch_opt.__doc__)
